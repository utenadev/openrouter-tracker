Title: Models | OpenRouter

URL Source: https://openrouter.ai/models?max_price=0&order=top-weekly

Markdown Content:
Models | OpenRouter
===============

[Skip to content](https://openrouter.ai/models?max_price=0&order=top-weekly#skip)

[OpenRouter](https://openrouter.ai/)

/

[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Rankings](https://openrouter.ai/rankings)[Enterprise](https://openrouter.ai/enterprise)[Pricing](https://openrouter.ai/pricing)[Docs](https://openrouter.ai/docs/quickstart)

Sign up

### Input Modalities

[Text](https://openrouter.ai/models?max_price=0&order=top-weekly&fmt=cards&input_modalities=text)

[Image](https://openrouter.ai/models?max_price=0&order=top-weekly&fmt=cards&input_modalities=image)

[File](https://openrouter.ai/models?max_price=0&order=top-weekly&fmt=cards&input_modalities=file)

[Audio](https://openrouter.ai/models?max_price=0&order=top-weekly&fmt=cards&input_modalities=audio)

[Video New](https://openrouter.ai/models?max_price=0&order=top-weekly&fmt=cards&input_modalities=video)

### Output Modalities

[Text](https://openrouter.ai/models?max_price=0&order=top-weekly&fmt=cards&output_modalities=text)

[Image](https://openrouter.ai/models?max_price=0&order=top-weekly&fmt=cards&output_modalities=image)

[Embeddings New](https://openrouter.ai/models?max_price=0&order=top-weekly&fmt=cards&output_modalities=embeddings)

### Context length

### Prompt pricing

### Series

### Categories

### Supported Parameters

### Distillable

New

### Providers

### Model Authors

Models
======

39 models

Reset Filters

Top Weekly Newest Top Weekly Pricing: Low to High Pricing: High to Low Context: High to Low Throughput: High to Low Latency: Low to High

Top Weekly Newest Top Weekly Pricing: Low to High Pricing: High to Low Context: High to Low Throughput: High to Low Latency: Low to High

*   [Xiaomi: MiMo-V2-Flash (free)MiMo-V2-Flash (free)](https://openrouter.ai/xiaomi/mimo-v2-flash:free) 385B tokens    Trivia (#1) Academia (#1) Finance (#1) Science (#1) Translation (#2)    +5 categories   [MiMo-V2-Flash is an open-source foundation language model developed by Xiaomi. It is a Mixture-of-Experts model with 309B total parameters and 15B active parameters, adopting hybrid attention architecture. MiMo-V2-Flash supports a hybrid-thinking toggle and a 256K context window, and excels at reasoning, coding, and agent scenarios. On SWE-bench Verified and SWE-bench Multilingual, MiMo-V2-Flash ranks as the top #1 open-source model globally, delivering performance comparable to Claude Sonnet 4.5 while costing only about 3.5% as much. Note: when integrating with agentic tools such as Claude Code, Cline, or Roo Code, **turn off reasoning mode** for the best and fastest performance—this model is deeply optimized for this scenario. Users can control the reasoning behaviour with the `reasoning``enabled` boolean. Learn more in our docs.](https://openrouter.ai/xiaomi/mimo-v2-flash:free)by [xiaomi](https://openrouter.ai/xiaomi)262K context$0/M input tokens$0/M output tokens        
*   [Mistral: Devstral 2 2512 (free)Devstral 2 2512 (free)](https://openrouter.ai/mistralai/devstral-2512:free) 110B tokens    Legal (#3) Academia (#4) Programming (#4)   [Devstral 2 is a state-of-the-art open-source model by Mistral AI specializing in agentic coding. It is a 123B-parameter dense transformer model supporting a 256K context window. Devstral 2 supports exploring codebases and orchestrating changes across multiple files while maintaining architecture-level context. It tracks framework dependencies, detects failures, and retries with corrections—solving challenges like bug fixing and modernizing legacy systems. The model can be fine-tuned to prioritize specific languages or optimize for large enterprise codebases. It is available under a modified MIT license.](https://openrouter.ai/mistralai/devstral-2512:free)by [mistralai](https://openrouter.ai/mistralai)262K context$0/M input tokens$0/M output tokens        
*   [Kwaipilot: KAT-Coder-Pro V1 (free)KAT-Coder-Pro V1 (free)](https://openrouter.ai/kwaipilot/kat-coder-pro:free) 103B tokens    Programming (#7) SEO (#10)   [KAT-Coder-Pro V1 is KwaiKAT's most advanced agentic coding model in the KAT-Coder series. Designed specifically for agentic coding tasks, it excels in real-world software engineering scenarios, achieving 73.4% solve rate on the SWE-Bench Verified benchmark. The model has been optimized for tool-use capability, multi-turn interaction, instruction following, generalization, and comprehensive capabilities through a multi-stage training process, including mid-training, supervised fine-tuning (SFT), reinforcement fine-tuning (RFT), and scalable agentic RL.](https://openrouter.ai/kwaipilot/kat-coder-pro:free)by [kwaipilot](https://openrouter.ai/kwaipilot)256K context$0/M input tokens$0/M output tokens        
*   [TNG: DeepSeek R1T2 Chimera (free)DeepSeek R1T2 Chimera (free)](https://openrouter.ai/tngtech/deepseek-r1t2-chimera:free) 86B tokens    Roleplay (#4) SEO (#8)   [DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. It is a 671 B-parameter mixture-of-experts text-generation model assembled from DeepSeek-AI’s R1-0528, R1, and V3-0324 checkpoints with an Assembly-of-Experts merge. The tri-parent design yields strong reasoning performance while running roughly 20 % faster than the original R1 and more than 2× faster than R1-0528 under vLLM, giving a favorable cost-to-intelligence trade-off. The checkpoint supports contexts up to 60 k tokens in standard use (tested to ~130 k) and maintains consistent <think> token behaviour, making it suitable for long-context analysis, dialogue and other open-ended generation tasks.](https://openrouter.ai/tngtech/deepseek-r1t2-chimera:free)by [tngtech](https://openrouter.ai/tngtech)164K context$0/M input tokens$0/M output tokens        
*   [Nex AGI: DeepSeek V3.1 Nex N1 (free)DeepSeek V3.1 Nex N1 (free)](https://openrouter.ai/nex-agi/deepseek-v3.1-nex-n1:free) 63B tokens    Roleplay (#7)   [DeepSeek V3.1 Nex-N1 is the flagship release of the Nex-N1 series — a post-trained model designed to highlight agent autonomy, tool use, and real-world productivity. Nex-N1 demonstrates competitive performance across all evaluation scenarios, showing particularly strong results in practical coding and HTML generation tasks.](https://openrouter.ai/nex-agi/deepseek-v3.1-nex-n1:free)by [nex-agi](https://openrouter.ai/nex-agi)131K context$0/M input tokens$0/M output tokens        
*   [TNG: DeepSeek R1T Chimera (free)DeepSeek R1T Chimera (free)](https://openrouter.ai/tngtech/deepseek-r1t-chimera:free) 20.5B tokens   [DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoning capabilities of R1 with the token efficiency improvements of V3. It is based on a DeepSeek-MoE Transformer architecture and is optimized for general text generation tasks. The model merges pretrained weights from both source models to balance performance across reasoning, efficiency, and instruction-following tasks. It is released under the MIT license and intended for research and commercial use.](https://openrouter.ai/tngtech/deepseek-r1t-chimera:free)by [tngtech](https://openrouter.ai/tngtech)164K context$0/M input tokens$0/M output tokens        
*   [NVIDIA: Nemotron 3 Nano 30B A3B (free)Nemotron 3 Nano 30B A3B (free)](https://openrouter.ai/nvidia/nemotron-3-nano-30b-a3b:free) 12.1B tokens   [NVIDIA Nemotron 3 Nano 30B A3B is a small language MoE model with highest compute efficiency and accuracy for developers to build specialized agentic AI systems. The model is fully open with open-weights, datasets and recipes so developers can easily customize, optimize, and deploy the model on their infrastructure for maximum privacy and security. Note: For the free endpoint, all prompts and output are logged to improve the provider's model and its product and services. Please do not upload any personal, confidential, or otherwise sensitive information. This is a trial use only. Do not use for production or business-critical systems.](https://openrouter.ai/nvidia/nemotron-3-nano-30b-a3b:free)by [nvidia](https://openrouter.ai/nvidia)256K context$0/M input tokens$0/M output tokens        
*   [Z.AI: GLM 4.5 Air (free)GLM 4.5 Air (free)](https://openrouter.ai/z-ai/glm-4.5-air:free) 7.07B tokens   [GLM-4.5-Air is the lightweight variant of our latest flagship model family, also purpose-built for agent-centric applications. Like GLM-4.5, it adopts the Mixture-of-Experts (MoE) architecture but with a more compact parameter size. GLM-4.5-Air also supports hybrid inference modes, offering a "thinking mode" for advanced reasoning and tool use, and a "non-thinking mode" for real-time interaction. Users can control the reasoning behaviour with the `reasoning``enabled` boolean. Learn more in our docs](https://openrouter.ai/z-ai/glm-4.5-air:free)by [z-ai](https://openrouter.ai/z-ai)131K context$0/M input tokens$0/M output tokens        
*   [NVIDIA: Nemotron Nano 12B 2 VL (free)Nemotron Nano 12B 2 VL (free)](https://openrouter.ai/nvidia/nemotron-nano-12b-v2-vl:free) 5.7B tokens   [NVIDIA Nemotron Nano 2 VL is a 12-billion-parameter open multimodal reasoning model designed for video understanding and document intelligence. It introduces a hybrid Transformer-Mamba architecture, combining transformer-level accuracy with Mamba’s memory-efficient sequence modeling for significantly higher throughput and lower latency. The model supports inputs of text and multi-image documents, producing natural-language outputs. It is trained on high-quality NVIDIA-curated synthetic datasets optimized for optical-character recognition, chart reasoning, and multimodal comprehension. Nemotron Nano 2 VL achieves leading results on OCRBench v2 and scores ≈ 74 average across MMMU, MathVista, AI2D, OCRBench, OCR-Reasoning, ChartQA, DocVQA, and Video-MME—surpassing prior open VL baselines. With Efficient Video Sampling (EVS), it handles long-form videos while reducing inference cost. Open-weights, training data, and fine-tuning recipes are released under a permissive NVIDIA open license, with deployment supported across NeMo, NIM, and major inference runtimes.](https://openrouter.ai/nvidia/nemotron-nano-12b-v2-vl:free)by [nvidia](https://openrouter.ai/nvidia)128K context$0/M input tokens$0/M output tokens        
*   [TNG: R1T Chimera (free)R1T Chimera (free)](https://openrouter.ai/tngtech/tng-r1t-chimera:free) 5.35B tokens   [TNG-R1T-Chimera is an experimental LLM with a faible for creative storytelling and character interaction. It is a derivate of the original TNG/DeepSeek-R1T-Chimera released in April 2025 and is available exclusively via Chutes and OpenRouter. Characteristics and improvements include: We think that it has a creative and pleasant personality. It has a preliminary EQ-Bench3 value of about 1305. It is quite a bit more intelligent than the original, albeit a slightly slower. It is much more think-token consistent, i.e. reasoning and answer blocks are properly delineated. Tool calling is much improved. TNG Tech, the model authors, ask that users follow the careful guidelines that Microsoft has created for their "MAI-DS-R1" DeepSeek-based model. These guidelines are available on Hugging Face (https://huggingface.co/microsoft/MAI-DS-R1).](https://openrouter.ai/tngtech/tng-r1t-chimera:free)by [tngtech](https://openrouter.ai/tngtech)164K context$0/M input tokens$0/M output tokens        
*   [Qwen: Qwen3 Coder 480B A35B (free)Qwen3 Coder 480B A35B (free)](https://openrouter.ai/qwen/qwen3-coder:free) 4.56B tokens   [Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts). Pricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.](https://openrouter.ai/qwen/qwen3-coder:free)by [qwen](https://openrouter.ai/qwen)262K context$0/M input tokens$0/M output tokens        
*   [DeepSeek: R1 0528 (free)R1 0528 (free)](https://openrouter.ai/deepseek/deepseek-r1-0528:free) 3.43B tokens   [May 28th update to the original DeepSeek R1 Performance on par with OpenAI o1, but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass. Fully open-source model.](https://openrouter.ai/deepseek/deepseek-r1-0528:free)by [deepseek](https://openrouter.ai/deepseek)164K context$0/M input tokens$0/M output tokens        

*   [Status](https://status.openrouter.ai/)
*   [Announcements](https://openrouter.ai/announcements)
*   [Docs](https://openrouter.ai/docs)
*   [Support](https://openrouter.ai/support)
*   [About](https://openrouter.ai/about)
*   [Partners](https://openrouter.ai/partners)
*   [Enterprise](https://openrouter.ai/enterprise)
*   [Careers](https://openrouter.ai/careers)
*   [Pricing](https://openrouter.ai/pricing)
*   [Privacy](https://openrouter.ai/privacy)
*   [Terms](https://openrouter.ai/terms)

© 2025 OpenRouter, Inc

[](https://discord.gg/fVyRaUDgxW)[](https://github.com/OpenRouterTeam)[](https://www.linkedin.com/company/104068329)[](https://twitter.com/openrouterai)
